{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Softmax Regression by using the Batch Gradient Descent with Early Stoppping, on the Iris dataset\n",
    "\n",
    "## Objective:\n",
    "\n",
    "* analyzing the relationship between the types of Iris and its petal length & width.\n",
    "* predicting the type of Iris based on its petal length and width.\n",
    "* realizing shorter run time by using early stopping.\n",
    "\n",
    "## Part 1: Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\antho\\AppData\\Local\\Temp\\ipykernel_11252\\900554092.py:11: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y = iris[\"target\"].astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "# loading modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# loading data\n",
    "iris = datasets.load_iris()\n",
    "list(iris.keys())\n",
    "X = iris[\"data\"][:, (2,3)] #petal length & width\n",
    "y = iris[\"target\"].astype(np.int)\n",
    "\n",
    "# adding bias term\n",
    "X = np.c_[np.ones([len(X),1]), X]\n",
    "\n",
    "# splitting training and test datasets\n",
    "def split_train_test(X, y, test_ratio, val_ratio):\n",
    "    test_size = int(len(X) * test_ratio)\n",
    "    val_size = int(len(X) * val_ratio)\n",
    "    train_size = len(X) - test_size - val_size\n",
    "    indices = np.random.permutation(len(X))\n",
    "    X_train = X[indices[:train_size]]\n",
    "    y_train = y[indices[:train_size]]\n",
    "    X_valid = X[indices[train_size:-test_size]]\n",
    "    y_valid = y[indices[train_size:-test_size]]\n",
    "    X_test = X[indices[-test_size:]]\n",
    "    y_test = y[indices[-test_size:]]\n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test\n",
    "X_train, y_train, X_valid, y_valid, X_test, y_test = split_train_test(X,y,0.2,0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the target has 3 classes, we need to encode them into a matrix containing a one-hot vector for each class, in order to train the Softmax Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y):\n",
    "    n_classes = y.max() + 1\n",
    "    m = len(y)\n",
    "    Y_one_hot = np.zeros((m, n_classes))\n",
    "    Y_one_hot[np.arange(m), y] = 1\n",
    "    return Y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 0, 1, 0, 0, 2, 1, 1, 0])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# demonstrating\n",
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_one_hot(y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Fitting the model\n",
    "\n",
    "Recall that the predicted probability by Softmax regression is calculated as follows:\n",
    "\n",
    "$$\\sigma(s(x))_k = \\frac{\\exp(s_k(x))}{\\sum_{j=1}^K \\exp(s_j(x))}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing softmax prediction funciton\n",
    "def softmax(logits):\n",
    "    exps = np.exp(logits)\n",
    "    exp_sums = np.sum(exps, axis=1, keepdims=True)\n",
    "    return exps / exp_sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function of Softmax regression is:\n",
    "\n",
    "$$\n",
    "J(\\Theta)=-\\frac{1}{m} \\sum_{i=1}^m \\sum_{k=1}^K y_k^{(i)}\\log(\\hat p_k^{(i)})\n",
    "$$\n",
    "\n",
    "and its gradient is:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta^{(k)}} J(\\Theta) = \\frac{1}{m} \\sum_{i=1}^m \\Big( \\hat p _k^{(i)} - y_k ^{(i)}\\Big)x^{(i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing the Batch Gradient Descent Algorithm\n",
    "def batch_gradient_descent(X_train, y_train, eta, n_iterations, epsilon):\n",
    "    n_inputs = X_train.shape[1]\n",
    "    n_outputs = len(np.unique(y_train))\n",
    "    Theta = np.random.randn(n_inputs, n_outputs)\n",
    "    y_train_one_hot = to_one_hot(y_train)\n",
    "    y_valid_one_hot = to_one_hot(y_valid)\n",
    "    y_test_one_hot = to_one_hot(y_test)\n",
    "    for iteration in range(n_iterations):\n",
    "        logits = X_train.dot(Theta)\n",
    "        y_proba = softmax(logits)\n",
    "        if iteration % 500 == 0:\n",
    "            loss = -np.mean(np.sum(y_train_one_hot * np.log(y_proba + epsilon), axis=1))\n",
    "        error = y_proba - y_train_one_hot\n",
    "        gradients = 1/len(X_train) * X_train.T.dot(error)\n",
    "        Theta = Theta - eta * gradients\n",
    "    return Theta\n",
    "\n",
    "Theta = batch_gradient_descent(X_train, y_train, 0.01, 5000, 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.32869753, -0.34527866, -4.54519084],\n",
       "       [-2.02548527, -0.77769323,  0.09708478],\n",
       "       [-0.61370951,  0.2207068 ,  0.3770351 ]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitted model:\n",
    "Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validating\n",
    "def val_accuracy(X_valid, y_valid, Theta):\n",
    "    logits = X_valid.dot(Theta)\n",
    "    Y_proba = softmax(logits)\n",
    "    y_predict = np.argmax(Y_proba, axis=1)\n",
    "    accuracy_score = np.mean(y_predict == y_valid)\n",
    "    return accuracy_score\n",
    "\n",
    "val_accuracy(X_valid, y_valid, Theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark: accuracy will change for each run, because the randomization seed is not fixed.\n",
    "\n",
    "So, let's see an overall accuracy over 100 runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.,  1.,  1.,  7.,  7., 19., 26., 42., 37., 58.]),\n",
       " array([0.66666667, 0.7       , 0.73333333, 0.76666667, 0.8       ,\n",
       "        0.83333333, 0.86666667, 0.9       , 0.93333333, 0.96666667,\n",
       "        1.        ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = []\n",
    "for counter in range(1, 200):\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = split_train_test(X,y,0.2,0.2)\n",
    "    Theta = batch_gradient_descent(X_train, y_train, 0.01, 5000, 1e-7)\n",
    "    temp = val_accuracy(X_valid, y_valid, Theta)\n",
    "    results.append(temp)\n",
    "\n",
    "plt.hist(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see this model is doing pretty well! But we can do better!\n",
    "\n",
    "Let's add the $\\ell_2$ regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.91174419,  1.14605674, -3.89234417],\n",
       "       [-1.07556068,  0.13235907,  0.94320162],\n",
       "       [-0.45328978, -0.14820346,  0.60149324]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implementing batch GD algorithm with l2 regularization\n",
    "def batch_gradient_descent_l2regularized(X_train, y_train, eta, n_iterations, epsilon, alpha): # alpha is regularization hyperparameter\n",
    "    n_inputs = X_train.shape[1]\n",
    "    n_outputs = len(np.unique(y_train))\n",
    "    y_train_one_hot = to_one_hot(y_train)\n",
    "    y_valid_one_hot = to_one_hot(y_valid)\n",
    "    y_test_one_hot = to_one_hot(y_test)\n",
    "    Theta = np.random.randn(n_inputs, n_outputs)\n",
    "    for iteration in range(n_iterations):\n",
    "        logits = X_train.dot(Theta)\n",
    "        Y_proba = softmax(logits)\n",
    "        if iteration % 500 == 0:\n",
    "            xentropy_loss = -np.mean(np.sum(y_train_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
    "            l2_loss = 1/2 * np.sum(np.square(Theta[1:]))\n",
    "            loss = xentropy_loss + alpha * l2_loss\n",
    "        error = Y_proba - y_train_one_hot\n",
    "        gradients = 1/len(X_train) * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]\n",
    "        Theta = Theta - eta * gradients\n",
    "    return Theta\n",
    "\n",
    "Theta = batch_gradient_descent_l2regularized(X_train, y_train, 0.1, 5000, 1e-7, 0.1)\n",
    "Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_accuracy(X_valid, y_valid, Theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.,  2.,  2.,  8., 10., 12., 38., 17., 46., 63.]),\n",
       " array([0.56666667, 0.61      , 0.65333333, 0.69666667, 0.74      ,\n",
       "        0.78333333, 0.82666667, 0.87      , 0.91333333, 0.95666667,\n",
       "        1.        ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = []\n",
    "for counter in range(1, 200):\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = split_train_test(X,y,0.2,0.2)\n",
    "    y_train_one_hot = to_one_hot(y_train)\n",
    "    y_valid_one_hot = to_one_hot(y_valid)\n",
    "    Theta = batch_gradient_descent_l2regularized(X_train, y_train, 0.01, 5000, 1e-7, 0.1)\n",
    "    temp = val_accuracy(X_valid, y_valid, Theta)\n",
    "    results.append(temp)\n",
    "\n",
    "plt.hist(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, let's add early stoppping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.65060297,  0.32876716, -1.9857755 ],\n",
       "       [-0.78361508,  0.23404325,  0.56287164],\n",
       "       [-0.40635665, -0.12996651,  0.54905098]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implementing the early stopping\n",
    "def batch_gradient_descent_l2regularized_earlystopping(X_train, y_train, eta, n_iterations, epsilon, alpha):\n",
    "    n_inputs = X_train.shape[1]\n",
    "    n_outputs = len(np.unique(y_train))\n",
    "    y_train_one_hot = to_one_hot(y_train)\n",
    "    y_valid_one_hot = to_one_hot(y_valid)\n",
    "    y_test_one_hot = to_one_hot(y_test)\n",
    "    best_loss = np.infty\n",
    "    Theta = np.random.randn(n_inputs, n_outputs)\n",
    "    for iteration in range(n_iterations):\n",
    "        logits = X_train.dot(Theta)\n",
    "        Y_proba = softmax(logits)\n",
    "        error = Y_proba - y_train_one_hot\n",
    "        gradients = 1/len(X_train) * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]\n",
    "        Theta = Theta - eta * gradients\n",
    "        logits = X_valid.dot(Theta)\n",
    "        Y_proba = softmax(logits)\n",
    "        xentropy_loss = -np.mean(np.sum(y_valid_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
    "        l2_loss = 1/2 * np.sum(np.square(Theta[1:]))\n",
    "        loss = xentropy_loss + alpha * l2_loss\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "        else:\n",
    "            break\n",
    "    return Theta\n",
    "\n",
    "Theta = batch_gradient_descent_l2regularized_earlystopping(X_train, y_train, 0.01, 5000, 1e-7, 0.1)\n",
    "Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  3.,   0.,   0.,   6.,   0.,   6.,  10.,  16.,  54., 104.]),\n",
       " array([0.13333333, 0.22      , 0.30666667, 0.39333333, 0.48      ,\n",
       "        0.56666667, 0.65333333, 0.74      , 0.82666667, 0.91333333,\n",
       "        1.        ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = []\n",
    "for counter in range(1, 200):\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = split_train_test(X,y,0.2,0.2)\n",
    "    y_train_one_hot = to_one_hot(y_train)\n",
    "    y_valid_one_hot = to_one_hot(y_valid)\n",
    "    Theta = batch_gradient_descent_l2regularized_earlystopping(X_train, y_train, 0.01, 5000, 1e-7, 0.1)\n",
    "    temp = val_accuracy(X_valid, y_valid, Theta)\n",
    "    results.append(temp)\n",
    "\n",
    "plt.hist(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = X_test.dot(Theta)\n",
    "Y_proba = softmax(logits)\n",
    "y_predict = np.argmax(Y_proba, axis=1)\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_test)\n",
    "accuracy_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f674cce88d736ed0af4f91f986332152988e4d080f9e8a754291934a46a3de6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
